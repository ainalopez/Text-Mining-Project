{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Loading the data and creating the corpus\n",
      "1.2 Computing the document term matrix and the tf idf matrix\n",
      "1.3 Applying the dictionary method\n",
      " \n",
      "1.4 Regression results\n",
      "1.4.1 Negative Score\n",
      "r-squared: 0.0442410148601\n",
      "slope: -1746.58980694\n",
      "intercept: 11836.9743377\n",
      "1.4.2 Positive Score\n",
      "r-squared: 0.0170780228909\n",
      "slope: 919.649383248\n",
      "intercept: 1214.24362952\n",
      "1.4.3 Type - Negative Score\n",
      "r-squared: 0.00822748121046\n",
      "slope: 1100.45205202\n",
      "intercept: 2387.14906056\n",
      "1.4.4 Type - Positive Score\n",
      "r-squared: 0.022356671439\n",
      "slope: 1537.32735334\n",
      "intercept: 2229.78078655\n",
      " \n",
      " \n",
      "SECOND PART: Cosine Similarity\n",
      "2.1 Load Corpus\n",
      "2.2 Computing TF IDF matrix\n",
      "2.3 SVD Decomposition\n",
      "2.4 Computing Cosine Similarity\n",
      " \n",
      "2.5 Results\n",
      "Good Reviews\n",
      "Without SVD:  0.0297159073674\n",
      "With SVD:  0.158796085077\n",
      " \n",
      "Bad Reviews\n",
      "Without SVD:  0.030714154158\n",
      "With SVD:  0.169015542019\n",
      " \n",
      "Good vs Bad Reviews\n",
      "Without SVD:  0.0199511013632\n",
      "With SVD:  0.114315738359\n",
      " \n",
      "Top Words\n",
      "Top 1.0 words\n",
      "[u'thi', u'food', u'place', u'order', u'servic', u'restaur', u'good', u'veri', u'tabl']\n",
      "Top 2.0 words\n",
      "[u'food', u'thi', u'place', u'good', u'order', u'veri', u'servic', u'restaur', u'tapa']\n",
      "Top 3.0 words\n",
      "[u'good', u'food', u'place', u'thi', u'order', u'veri', u'servic', u'restaur', u'tapa']\n",
      "Top 4.0 words\n",
      "[u'good', u'thi', u'place', u'food', u'veri', u'great', u'tapa', u'barcelona', u'order']\n",
      "Top 5.0 words\n",
      "[u'thi', u'place', u'food', u'great', u'barcelona', u'good', u'veri', u'best', u'tapa']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/ainalopez/Desktop/Text Mining Project/Data/\"\n",
    "stopwords_dir = '/Users/ainalopez/Downloads/text_mining-master-2/data/stopwords/stopwords.txt'\n",
    "dict_dir = \"/Users/ainalopez/Downloads/dictionary1.xlsx\"\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "import math \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from itertools import repeat\n",
    "from scipy import stats\n",
    "import pylab\n",
    "\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2], doc[3]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "     \n",
    "    \n",
    "    \n",
    "    def document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of frequency counts\n",
    "        \"\"\"\n",
    "        # subroutine: computes the counts of each vocabulary in the document\n",
    "        def counts(doc):\n",
    "            # initialize a matrix\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1\n",
    "            return term_mat;\n",
    "        \n",
    "        self.doc_term_matrix = [list(self.token_set)]\n",
    "        \n",
    "        for doc in self.docs:\n",
    "            self.doc_term_matrix.append(counts(doc))\n",
    "            \n",
    "         \n",
    "        \n",
    "    def tf_idf(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of tf-idf scores\n",
    "        \"\"\"\n",
    "        # Compute inverse document frequency\n",
    "        idf = [0]*len(self.token_set)\n",
    "        for token in self.token_set:\n",
    "            ind = 0\n",
    "            for doc in self.docs:\n",
    "                if token in doc.tokens:\n",
    "                    ind += 1\n",
    "                    idf[list(self.token_set).index(token)] = math.log(self.N/ind)\n",
    "\n",
    "        # Create a subroutine that computes tf_idf for one document\n",
    "        def tfidf(doc):\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1\n",
    "            \n",
    "            for i,term in enumerate(term_mat):\n",
    "                if term != 0:\n",
    "                    term_mat[i] = (1 + math.log(term)) * idf[i]\n",
    "            return term_mat;\n",
    "    \n",
    "        #tf_idf\n",
    "        self.tf_idf_matrix = [list(self.token_set)]\n",
    "        for doc in self.docs:\n",
    "            self.tf_idf_matrix.append( tfidf(doc))\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, restaurant_name, restaurant_price,review_score, review_text):\n",
    "        self.name = restaurant_name\n",
    "        self.price = restaurant_price\n",
    "        self.score = review_score\n",
    "        self.text = review_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "\n",
    "\n",
    "print \"1.1 Loading the data and creating the corpus\"\n",
    "\n",
    "# Load the yelp data\n",
    "yelp_reviews = list()\n",
    "restaurant_tags = list()\n",
    "restaurant_type = list()\n",
    "restaurant_score = list()\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "   f = open(data_dir + file)\n",
    "   f = json.load(f)\n",
    "   try: \n",
    "        yelp_reviews.append(list([f[6], f[9], f[1], f[0] ]))\n",
    "        restaurant_tags.append(f[8])\n",
    "        restaurant_score.append(f[1])\n",
    "        restaurant_type.append(f[9])\n",
    "   except:\n",
    "        next \n",
    "        \n",
    "# Create the Corpus     \n",
    "corpus = Corpus(yelp_reviews, stopwords_dir, 2)\n",
    "\n",
    "\n",
    "print \"1.2 Computing the document term matrix and the tf idf matrix\"\n",
    "# Create the matrices: document term and tf_idf. \n",
    "corpus.document_term_matrix()\n",
    "corpus.tf_idf()\n",
    "\n",
    "\n",
    "print \"1.3 Applying the dictionary method\"\n",
    "# Load Dictionaries\n",
    "df = pd.read_excel(dict_dir, skiprows=0)\n",
    "w = df['Word']\n",
    "words = [str(x).lower() for x in df['Word']]\n",
    "words = [PorterStemmer().stem(t) for t in words]\n",
    "\n",
    "score1 = [str(x).lower() for x in df['Positive']] \n",
    "dictionary1 = dict(zip(words,score1))\n",
    "\n",
    "score2 = [str(x).lower() for x in df['Negative']] \n",
    "dictionary2 = dict(zip(words,score2))\n",
    "\n",
    "score_pos = []\n",
    "x = corpus.tf_idf_matrix[0]\n",
    "X = corpus.tf_idf_matrix[1:]\n",
    "for token in x: \n",
    "    try:\n",
    "        score_pos.append(dictionary1[token])\n",
    "    except: \n",
    "        score_pos.append(0)\n",
    "        \n",
    "# get a vector with all the scores in order\n",
    "score_pos=[int(x) for x in score_pos]\n",
    "rank_pos = {}\n",
    "elements=range(len(X))\n",
    "   \n",
    "for i in elements:\n",
    "    rank_pos[i] = np.dot(score_pos,X[i])\n",
    "    \n",
    "x = corpus.tf_idf_matrix[0]    \n",
    "score_neg = []\n",
    "for token in x: \n",
    "    try:\n",
    "        score_neg.append(dictionary2[token])\n",
    "    except: \n",
    "        score_neg.append(0)\n",
    "        \n",
    "# get a vector with all the scores in order\n",
    "score_neg =[int(x) for x in score_neg]\n",
    "rank_neg = {}\n",
    "elements=range(len(X))\n",
    "   \n",
    "for i in elements:\n",
    "    rank_neg[i] = np.dot(score_neg,X[i])\n",
    "    \n",
    "scores =  map(float, np.asarray(restaurant_score))\n",
    "\n",
    "print \" \"\n",
    "print \"1.4 Regression results\"\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(scores,np.asarray(rank_neg.values()))\n",
    "print \"1.4.1 Negative Score\"\n",
    "print \"r-squared:\", r_value**2\n",
    "print \"slope:\", slope\n",
    "print \"intercept:\", intercept\n",
    "\n",
    "\n",
    "slope2, intercept2, r_value2, p_value2, std_err2 = stats.linregress(scores,np.asarray(rank_pos.values()))\n",
    "print \"1.4.2 Positive Score\"\n",
    "print \"r-squared:\", r_value2**2\n",
    "print \"slope:\", slope2\n",
    "print \"intercept:\", intercept2\n",
    "\n",
    "\n",
    "predict_neg = intercept + slope * np.array([1,2,3,4,5])\n",
    "pylab.figure()\n",
    "# Plotting\n",
    "pylab.plot(scores, np.asarray(rank_neg.values()), 'o')\n",
    "pylab.plot(np.array([1,2,3,4,5]), predict_neg, 'k-')\n",
    "pylab.xlabel('Review Score')\n",
    "pylab.ylabel('Negative Dictionary Score')\n",
    "pylab.title('Negative Dictionary Regression')\n",
    "pylab.savefig('negative.png')\n",
    "\n",
    "\n",
    "predict_pos = intercept2 + slope2 * np.array([1,2,3,4,5])\n",
    "\n",
    "# Plotting\n",
    "pylab.figure()\n",
    "pylab.plot(scores, np.asarray(rank_pos.values()), 'o')\n",
    "pylab.plot(np.array([1,2,3,4,5]), predict_pos, 'k-')\n",
    "pylab.xlabel('Review Score')\n",
    "pylab.ylabel('Positive Dictionary Score')\n",
    "pylab.title('Positive Dictionary Regression')\n",
    "pylab.savefig('positive.png')\n",
    "\n",
    "scores =  map(float, np.asarray(restaurant_type))\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(scores,np.asarray(rank_neg.values()))\n",
    "print \"1.4.3 Type - Negative Score\"\n",
    "print \"r-squared:\", r_value**2\n",
    "print \"slope:\", slope\n",
    "print \"intercept:\", intercept\n",
    "\n",
    "\n",
    "slope2, intercept2, r_value2, p_value2, std_err2 = stats.linregress(scores,np.asarray(rank_pos.values()))\n",
    "print \"1.4.4 Type - Positive Score\"\n",
    "print \"r-squared:\", r_value2**2\n",
    "print \"slope:\", slope2\n",
    "print \"intercept:\", intercept2\n",
    "\n",
    "\n",
    "predict_neg = intercept + slope * np.array([1,2,3,4])\n",
    "\n",
    "# Plotting\n",
    "pylab.figure()\n",
    "pylab.plot(scores, np.asarray(rank_neg.values()), 'o')\n",
    "pylab.plot(np.array([1,2,3,4]), predict_neg, 'k-')\n",
    "pylab.xlabel('Restaurant Type')\n",
    "pylab.ylabel('Negative Dictionary Score')\n",
    "pylab.title('Negative Dictionary Regression')\n",
    "pylab.savefig('negative_2.png')\n",
    "        \n",
    "\n",
    "predict_pos = intercept2 + slope2 * np.array([1,2,3,4,5])\n",
    "\n",
    "# Plotting\n",
    "pylab.figure()\n",
    "pylab.plot(scores, np.asarray(rank_pos.values()), 'o')\n",
    "pylab.plot(np.array([1,2,3,4,5]), predict_pos, 'k-')\n",
    "pylab.xlabel('Restaurant Type')\n",
    "pylab.ylabel('Positive Dictionary Score')\n",
    "pylab.title('Positive Dictionary Regression')\n",
    "pylab.savefig('positive_2.png')\n",
    "\n",
    "print \" \"\n",
    "print \" \"\n",
    "print \"SECOND PART: Cosine Similarity\"\n",
    "\n",
    "# Load the yelp data\n",
    "yelp_reviews = list()\n",
    "restaurant_score = list()\n",
    "i = 0\n",
    "\n",
    "\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "   f = open(data_dir + file)\n",
    "   f = json.load(f)\n",
    "   try: \n",
    "        if f[1] == \"1.0\":\n",
    "            yelp_reviews.append(list([f[6], f[9], f[1], f[0] ]))\n",
    "            restaurant_score.append(f[1])\n",
    "   except:\n",
    "        next \n",
    "                \n",
    "for file in os.listdir(data_dir):\n",
    "   f = open(data_dir + file)\n",
    "   f = json.load(f)\n",
    "   try: \n",
    "        if i >= 500:\n",
    "            break\n",
    "        if f[1] == \"5.0\":\n",
    "            yelp_reviews.append(list([f[6], f[9], f[1], f[0] ]))\n",
    "            restaurant_score.append(f[1]) \n",
    "            i = i+1\n",
    "\n",
    "   except:\n",
    "        next \n",
    "\n",
    "print \"2.1 Load Corpus\"\n",
    "        \n",
    "# Create the Corpus     \n",
    "corpus2 = Corpus(yelp_reviews, stopwords_dir, 2)\n",
    "\n",
    "\n",
    "print \"2.2 Computing TF IDF matrix\"\n",
    "# Create the matrices: document term and tf_idf. \n",
    "corpus2.document_term_matrix()\n",
    "corpus2.tf_idf()\n",
    "\n",
    "\n",
    "\n",
    "print \"2.3 SVD Decomposition\"\n",
    "X = corpus2.tf_idf_matrix[1:]\n",
    "\n",
    "# Compute svd \n",
    "sing_values_nb = 120\n",
    " \n",
    "U, s, V = np.linalg.svd(X)\n",
    "X_hat = np.dot(U[:,0:(sing_values_nb-1)] * s[0:(sing_values_nb-1)], V[0:(sing_values_nb-1),:])\n",
    "\n",
    "print \"2.4 Computing Cosine Similarity\"\n",
    "\n",
    "def cosine_similarity(doc1, doc2):\n",
    "    return np.dot(doc1, doc2) / ( math.sqrt(np.dot(doc1, doc1))* math.sqrt(np.dot(doc2, doc2)) )\n",
    "\n",
    "similarity_X = np.zeros((len(X), len(X)))\n",
    "similarity_X_hat = np.zeros((len(X_hat), len(X_hat)))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X)):\n",
    "        similarity_X[i][j] = cosine_similarity(X[i], X[j])\n",
    "        \n",
    "for i in range(len(X_hat)):\n",
    "    for j in range(len(X_hat)):\n",
    "        similarity_X_hat[i][j] = cosine_similarity(X_hat[i], X_hat[j])\n",
    "\n",
    "        \n",
    "bad_index = []\n",
    "good_index = []\n",
    "D = []\n",
    "\n",
    "for i in range(len(X_hat)):\n",
    "    if restaurant_score[i] == \"1.0\":\n",
    "        good_index.append(i)\n",
    "    if restaurant_score[i] == \"5.0\":\n",
    "        bad_index.append(i)\n",
    "\n",
    "\n",
    "good_good = []\n",
    "bad_bad = []\n",
    "bad_good = []\n",
    "\n",
    "good_good_svd = []\n",
    "bad_bad_svd = []\n",
    "bad_good_svd = []\n",
    "\n",
    "\n",
    "for r in bad_index:\n",
    "    bad_bad.extend(similarity_X[r, bad_index])\n",
    "    bad_bad_svd.extend(similarity_X_hat[r, bad_index])\n",
    "    bad_good.extend(similarity_X[r, good_index])\n",
    "    bad_good_svd.extend(similarity_X_hat[r, good_index])\n",
    "    \n",
    "for d in good_index:\n",
    "    good_good.extend(similarity_X[d, good_index])\n",
    "    good_good_svd.extend(similarity_X_hat[d, good_index])\n",
    "    bad_good.extend(similarity_X[d, bad_index])\n",
    "    bad_good_svd.extend(similarity_X_hat[d, bad_index])\n",
    "\n",
    "print \" \"\n",
    "print \"2.5 Results\"\n",
    "print \"Good Reviews\"\n",
    "print \"Without SVD: \", np.mean(good_good)\n",
    "print \"With SVD: \", np.mean(good_good_svd)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print \"Bad Reviews\"\n",
    "print \"Without SVD: \",np.mean(bad_bad) \n",
    "print \"With SVD: \",np.mean(bad_bad_svd)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print \"Good vs Bad Reviews\"\n",
    "print \"Without SVD: \", np.mean(bad_good)\n",
    "print \"With SVD: \", np.mean(bad_good_svd)\n",
    "\n",
    "\n",
    "\n",
    "print ' '\n",
    "print 'Top Words'\n",
    "\n",
    "one = []\n",
    "two = []\n",
    "three = []\n",
    "four = []\n",
    "five = []\n",
    "\n",
    "for doc in corpus.docs:\n",
    "    if doc.score ==\"1.0\":\n",
    "        for token in doc.tokens:\n",
    "            one.append(token)\n",
    "    if doc.score ==\"2.0\":\n",
    "        for token in doc.tokens:\n",
    "            two.append(token)\n",
    "    if doc.score ==\"3.0\":\n",
    "        for token in doc.tokens:\n",
    "            three.append(token)\n",
    "    if doc.score ==\"4.0\":\n",
    "        for token in doc.tokens:\n",
    "            four.append(token)\n",
    "    if doc.score ==\"5.0\":\n",
    "        for token in doc.tokens:\n",
    "            five.append(token)\n",
    "\n",
    "\n",
    "         \n",
    "fd1 = nltk.FreqDist(one)\n",
    "d1 = {}\n",
    "\n",
    "for token in fd1:\n",
    "     d1[token]= fd1[token]\n",
    "\n",
    "\n",
    "fd2 = nltk.FreqDist(two)\n",
    "d2 = {}\n",
    "\n",
    "for token in fd2:\n",
    "     d2[token]= fd2[token]\n",
    "        \n",
    "fd3 = nltk.FreqDist(three)\n",
    "d3 = {}\n",
    "\n",
    "for token in fd3:\n",
    "     d3[token]= fd3[token]\n",
    "        \n",
    "        \n",
    "fd4 = nltk.FreqDist(four)\n",
    "d4 = {}\n",
    "\n",
    "for token in fd4:\n",
    "     d4[token]= fd4[token]\n",
    "        \n",
    "fd5 = nltk.FreqDist(five)\n",
    "d5 = {}\n",
    "\n",
    "for token in fd5:\n",
    "     d5[token]= fd5[token]\n",
    "\n",
    "print 'Top 1.0 words'\n",
    "print sorted(d1, key=d1.get, reverse=True)[1:10] \n",
    "\n",
    "print 'Top 2.0 words'\n",
    "print sorted(d2, key=d2.get, reverse=True)[1:10] \n",
    "\n",
    "print 'Top 3.0 words'\n",
    "print sorted(d3, key=d3.get, reverse=True)[1:10] \n",
    "\n",
    "print 'Top 4.0 words'\n",
    "print sorted(d4, key=d4.get, reverse=True)[1:10] \n",
    "\n",
    "print 'Top 5.0 words'\n",
    "print sorted(d5, key=d5.get, reverse=True)[1:10] \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
